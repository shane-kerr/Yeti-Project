<?xml version="1.0"?>
<!-- This template is for creating an Internet Draft using xml2rfc,
     which is available here: http://xml.resource.org. -->
<!DOCTYPE rfc SYSTEM "rfc2629.dtd" [
<!-- One method to get references from the online citation libraries.
     There has to be one entity for each item to be referenced.
     An alternate method (rfc include) is described in the references. -->

<!ENTITY RFC1035 SYSTEM "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.1035.xml">
<!ENTITY RFC1123 SYSTEM "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.1123.xml">
<!ENTITY RFC3542 SYSTEM "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.3542.xml">
<!ENTITY RFC5681 SYSTEM "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.5681.xml">
<!ENTITY RFC6891 SYSTEM "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.6891.xml">
<!ENTITY I-D.dnsop-cookies SYSTEM "http://xml2rfc.ietf.org/public/rfc/bibxml3/reference.I-D.draft-ietf-dnsop-cookies-02.xml">
<!ENTITY I-D.dnsop-respsize SYSTEM "http://xml2rfc.ietf.org/public/rfc/bibxml3/reference.I-D.draft-ietf-dnsop-respsize-15.xml">
]>
<?xml-stylesheet type='text/xsl' href='rfc2629.xslt' ?>
<!-- used by XSLT processors -->
<!-- For a complete list and description of processing instructions (PIs),
     please see http://xml.resource.org/authoring/README.html. -->
<!-- Below are generally applicable Processing Instructions (PIs) that most I-Ds might want to use.
     (Here they are set differently than their defaults in xml2rfc v1.32) -->
<?rfc strict="yes" ?>
<!-- give errors regarding ID-nits and DTD validation -->
<!-- control the table of contents (ToC) -->
<?rfc toc="yes"?>
<?rfc tocappendix="yes"?>
<!-- generate a ToC -->
<?rfc tocdepth="3"?>
<!-- the number of levels of subsections in ToC. default: 3 -->
<!-- control references -->
<?rfc symrefs="yes"?>
<!-- use symbolic references tags, i.e, [RFC2119] instead of [1] -->
<?rfc sortrefs="yes" ?>
<!-- sort the reference entries alphabetically -->
<!-- control vertical white space
     (using these PIs as follows is recommended by the RFC Editor) -->
<?rfc compact="yes" ?>
<!-- do not start each main section on a new page -->
<?rfc subcompact="no" ?>
<!-- keep one blank line between list items -->
<!-- end of list of popular I-D processing instructions -->
<?rfc comments="no" ?>
<?rfc inline="yes" ?>
<rfc category="exp" docName="draft-song-yeti-testbed-experience-00" ipr="trust200902">

  <front>

    <title>Experiences from Root Testbed in the Yeti DNS Project</title>

    <author fullname="Linjian Song" initials="L." surname="Song">
      <organization>Beijing Internet Institute</organization>
      <address>
        <postal>
          <street>2508 Room, 25th Floor, Tower A, Time Fortune</street>
          <city>Beijing</city>
          <region></region>
          <code>100028</code>
          <country>P. R. China</country>
        </postal>
        <email>songlinjian@gmail.com</email>
        <uri>http://www.biigroup.com/</uri>
      </address>
    </author>

    <date/>

    <!-- Meta-data Declarations -->

    <area>Internet Area</area>
    <workgroup>Internet Engineering Task Force</workgroup>

    <!-- <keyword>dns</keyword> -->

    <abstract>
      <t>This document reports and discusses issues on advanced root 
      	services through our experiences from the experiments in Yeti 
      	DNS project, including IPv6-only operation, root naming scheme 
      	, KsK rollover, root renumbering, Multiple Zone signer etc. This 
      	project founded in May 2015 and build a live root DNS server 
      	system testbed with volunteer root servers and resolvers.</t>
    </abstract>

  </front>

  <middle>

    <section title="Introduction">
    
    <t>The top level of the unique identifier system, the DNS root system, 
    has been operational for 25+ years. It is pivot to make the current 
    Internet useful. So it is considered somewhat ossified for stability 
    reasons. It is hard to test and implement new ideas evolving to a 
    more advanced level to counter challenges like IPv6-only operation, 
    DNSSEC key/algorithm rollover, scaling issues, etc. In order to make 
    the test more practical, it is also necessary to involve users’ 
    environment which is highly diversified, to study the effect of the 
    changes in question.</t>

    <t>To benefit the Internet development as a whole, the proposal of Yeti 
    Project is formed to build a parallel experimental live IPv6 DNS root 
    system to discover the limits of DNS root name service and deliver 
    useful technical output. Possible research agenda will be explored on 
    this testbed covering several aspects but not limited to:</t>

  <t> <list style="symbols">
	<t>IPv6-only operation</t>
	<t>DNSSEC key rollover</t>
	<t>Renumbering issues</t>
	<t>Scalability issues</t>
	<t>Multiple zone file signers</t>
  </list></t>
    <t>Starting from May 2015, three coordinators began to build this live 
    experimental environment and call for participants. At the time of 
    writing, there are 14 Yeti root servers with 13 operators, and experimental 
    traffic from vunlentee Universities, DNS vendors, Mirrored traffic and 
    Atlas probes. Some experiments are proposed and verified in lab test.</t>


	<t> Note that the Yeti DNS project has complete fealty to IANA as the DNS 
	name space manager. All IANA top-level domain names will be precisely 
	expressed in the Yeti DNS system, including all TLD data and meta-data. 
	So, the Yeti DNS project is not an “alternative root” in any sense of 
	that term. We hope to inform the IANA community by peer-reviewed science 
	as to future possibilities to consider for the IANA root DNS system </t>
	
	<t> In order to let me people know the technical activities in Yeti DNS 
	project, this document reports and discusses issues on advanced root services 
	through our experiences so far from the experiments in Yeti DNS project.
	This document will continue to be updated before the project ends</t>

    </section>

    <section title="Problem Statement">

    <t>Some problems and policy concerns over the DNS Root Server system stem 
    from centralization from the point of view of DNS content consumers. These 
    include external dependencies and surveillance threats.</t>

    <t><list style="symbols">
	<t>External Dependency. Currently, there are 12 DNS Root Server operators for
	 the 13 Root Server letters, with more than 500 instances deployed globally. 
	 Compared to the number of connected devices, AS networks, and recursive DNS 
	 servers, the number of root instances is far from sufficient. Connectivity 
	 loss between one autonomous network and the IANA root name servers usually 
	 results in loss of local service within the local network, even when internal 
	 connectivity is perfect. </t>

	<t>Surveillance risk. Even when one or more root name server anycast instances 
	are deployed locally or in a nearby network, the queries sent to the root 
	servers carry DNS lookup information which enables root operators or other 
	parties to analyze the DNS query traffic. This is a kind of information 
	leakage which is to some extent not acceptable to some policy makers.</t>
	</list></t>

	<t>People are always told that current root system with 13 root severs is not 
	able to be extended to enable more regional operators to run its own root servers
	alleviating above concerns. To the best of authors' knowledge, there is no 
	such scientific evidence to the question. It is still unknown.</t>

	<t>There are some technical issues in the areas of IPv6 and DNSSEC, which were 
	introduced to the DNS Root Server system after it was created, and also when 
	renumbering DNS Root Servers.</t>

	<t><list style="symbols">
	<t>IPv6-only capability. Currently Some DNS servers which support both 
	A and AAAA (IPv4 and IPv6) records still do not respond to IPv6 queries. IPv6 
	introduces larger IP packet MTU (1280 bytes) and a different fragmentation 
	model. It is not clear whether it can survive without IPv4 (in an IPv6-only 
	environment), or what the impact of IPv6-only environment introduces to 
	current DNS operations especially in the DNS Root Server system.</t>

	<t>KSK rollover.  Currently, IANA rolls the ZSK every six weeks but the 
	KSK has never been rolled as of writing. Is the 512 bytes DNS packet size 
	limitation still observed? Is RFC5011 widely supported by resolvers? How 
	about longer key with different encryption algorithm ？There are many issues 
	still unknown. </t>

	<t>Renumbering issue. It is likely that root operators may change their IP 
	addresses for root servers as well. Since there is no dynamic update mechanism 
	to inform resolvers and other Internet infrastructure relying on root service 
	of such changes.</t>
	</list></t>
    </section>
  
    <section title="Yeti testbed and experiment setup">
    
    
    <t>To make the Yeti testbed operationally ready, the cooperation perimeter 
    that's required for correct root name service is a matching set of the following:</t>

  <t> <list style="symbols">
	<t>a root "hints file"</t>
	<t>the root zone apex NS record set</t>
	<t>the root zone's signing key</t>
	<t>root zone trust anchor.</t>
  </list></t>
	<t>Although Yeti DNS project publishes strictly IANA information for TLD 
	data and meta-data, it's necessary to use a special hint file and replace 
	the apex NS RRset with Yeti authority name servers, which will enable the 
	resolves to find and stick to the Yeti root system. In addition, unless IANA 
	help Yeti sign its root zone with a different root set, it is necessary to keep 
	using another ZSK and trust anchor in Yeti system.</t>

	<t>Blow is a figure to demonstrate the topology of Yeti and basic data flow which 
    mainly consist by Yeti distribution master, Yeti root server and Yeti resolver : </t>


	<figure>
	<artwork> 
	<![CDATA[ 

	                +------------------------+
                        |   IANA Root Zone via   |
                      +-+   F.root-servers.net   +--+
                      | +-----------+------------+  |
+-----------+         |             |               | IANA Root.Zone
|    Yeti   |         |             |               |
|  Traffic  |      +--v---+     +---v--+      +-----v+
| Collection|      |  BII |     | WIDE |      | TISF |
|           |      |  DM  |     |  DM  |      |  DM  |
+---+----+--+      +------+     +-+----+      +---+--+
    ^    ^         |              |               |
    |    |         |              |               |   Yeti Root.Zone
    |              v              v               v
         |
    |        +------+      +------+                +------+
         +- -+ Yeti |      | Yeti |   .....        | Yeti |
    |        | Root |      | Root |                | Root |
             +---+--+      +---+--+                +--+---+
    |            |             |                      |
      pcap       |             |                      |  TLD lookup
    | upload     v             v                      v
     
    |                   +--------------------------+
    +- - - - - - - - - -+      Yeti Resolvers      |
                        |     (with Yeti Hint)     |
                        +--------------------------+



	]]></artwork>
    </figure>
	<t>Figure 1. The topology of Yeti testbed</t>

    <section title="Distribution master">

	<t>Show in figure 1, the Yeti Root system takes the IANA root zone, and performs 
	minimal changes needed to serve the zone from the Yeti root servers instead of 
	the IANA root servers. In Yeti, this modified root zone is generated by the Yeti 
	Distribution Masters (DM), which provide it to the Yeti root servers.</t>

	<t>So the generation process is: </t>
  <t> <list style="symbols">
	<t>DM Download the latest IANA root zone at a certain time</t>
	<t>Make modifications to change from the IANA to Yeti root servers</t>
	<t>Sign the new Yeti root zone</t>
	<t>Publish the new Yeti root zone to Yeti root servers</t>
  </list></t>
  <t>While in principle this could be done by a single DM, but Yeti uses a set of three 
  DM to avoid any of them being taken over. The tree Distribution Masters (DM) who can 
  independently fetch the root zone from IANA, sign it and publish the latest Zone data 
  to Yeti root server. </t>

  <t>In the same while, these DM coordinate their work so that the resulting Yeti root zone 
    is always consistent. There are two apsects of coordination between three DMs: Timing 
    and information Synchronization.</t>


  <section title="Timing to fetch the zone">
  <t>Yeti root system operators do not receive notify massage from IANA when IANA 
    root zone update with a new serial number. So Yeti DMs should check the root zone 
    periodically.  At the time of writing, each Yeti DM checks to see if the IANA root 
    zone has changed hourly, on the following schedule:</t>

    <texttable>

    <ttcol>DM Operator</ttcol>
    <ttcol>Time</ttcol>
    <c>BII</c><c>hour+00</c>
    <c>WIDE</c><c>hour+20</c>
    <c>TISF</c><c>hour+40</c>
  </texttable>

  <t>Once the IANA root zone has changed with a new serial number, a new version of the 
    Yeti root zone is generated with the same serial number.</t>
  </section>

    <section title="Information synchronization">

  <t>Given three DMs operational in Yeti root system, it is necessary to prevent any inconsistency 
    caused by human mistakes in operation. The straight method is to share the same parameters to 
    produce the Yeti root zone. There parameters includes following set of files: </t>

    <t> <list style="symbols">
    <t>the list of Yeti root servers, including:
      <list style="symbols">
      <t>public IPv6 address host name </t>
      <t>IPv6 addresses originating zone transfer</t>
      <t>IPv6 addresses to send DNS notify to</t>
      </list></t>
    <t>the ZSK used to sign the root</t>
    <t>the KSK used to sign the root</t>
    <t>the serial when this information is active</t>
  </list></t>

  <t>The theory of operation is straight that each DM operator runs a Git repository, 
    containing files with the information needed to produce the Yeti root zone. When a 
    change is desired(adding a new serer or roll ZSK), a DM operator updates the local 
    Git repository. A serial number in the future is chosen for when the changes become 
    active. The DM operator then pushes the changes to the Git repositories of the other 
    two DM operators. When the serial of the root zone passes the number chosen, then the 
    new version of the information is used.</t>
    </section>
  </section>
    	
    	<section title="Yeti Root Servers">

      <t>In Yeti Root system, authoritative servers donated by Yeti volunteers are configured 
        as a slave to the Yeti DM. As the time of writing, there are 14 Yeti root servers 
        distributed around the world. As one of operational research goal, all authoritative 
        servers are required to work in IPv6 only environment.</t>

          <figure>
  <artwork> 
    <![CDATA[
.                              3600000    IN   NS       bii.dns-lab.net                         
bii.dns-lab.net                3600000    IN   AAAA     240c:f:1:22::6                          
.                              3600000    IN   NS       yeti-ns.tisf.net                        
yeti-ns.tisf.net               3600000    IN   AAAA     2001:559:8000::6                        
.                              3600000    IN   NS       yeti-ns.wide.ad.jp                      
yeti-ns.wide.ad.jp             3600000    IN   AAAA     2001:200:1d9::35                        
.                              3600000    IN   NS       yeti-ns.as59715.net                     
yeti-ns.as59715.net            3600000    IN   AAAA     2a02:cdc5:9715:0:185:5:203:53           
.                              3600000    IN   NS       dahu1.yeti.eu.org                       
dahu1.yeti.eu.org              3600000    IN   AAAA     2001:4b98:dc2:45:216:3eff:fe4b:8c5b     
.                              3600000    IN   NS       ns-yeti.bondis.org                      
ns-yeti.bondis.org             3600000    IN   AAAA     2a02:2810:0:405::250                    
.                              3600000    IN   NS       yeti-ns.ix.ru                           
yeti-ns.ix.ru                  3600000    IN   AAAA     2001:6d0:6d06::53                       
.                              3600000    IN   NS       yeti.bofh.priv.at                       
yeti.bofh.priv.at              3600000    IN   AAAA     2a01:4f8:161:6106:1::10                 
.                              3600000    IN   NS       yeti.ipv6.ernet.in                      
yeti.ipv6.ernet.in             3600000    IN   AAAA     2001:e30:1c1e:1::333                    
.                              3600000    IN   NS       yeti-dns01.dnsworkshop.org              
yeti-dns01.dnsworkshop.org     3600000    IN   AAAA     2001:1608:10:167:32e::53                
.                              3600000    IN   NS       yeti-ns.conit.co                        
yeti-ns.conit.co               3600000    IN   AAAA     2607:ff28:2:10::47:a010                 
.                              3600000    IN   NS       dahu2.yeti.eu.org                       
dahu2.yeti.eu.org              3600000    IN   AAAA     2001:67c:217c:6::2                      
.                              3600000    IN   NS       yeti.aquaray.com                        
yeti.aquaray.com               3600000    IN   AAAA     2a02:ec0:200::1                         
.                              3600000    IN   NS       yeti-ns.switch.ch                       
yeti-ns.switch.ch              3600000    IN   AAAA     2001:620:0:ff::29    
  ]]>
  </artwork>
</figure>
<t>Figure 2. the yeti root server in hint file </t>

      <t>Since Yeti is a scientific research project, it needs to capture DNS traffic sent for 
        later analysis. Today this is done using dnscap, which is a DNS-specific tool to produce 
        pcap files. There are several versions of dnscap floating around, some people use the V
        erisign one. Since dnscap loses packets in some cases (tested on a Linux kernel), some 
        people use pcapdump. It requires the patch attached to this bug report [https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=545985] </t>

      <t>System diversity is also a requirement and observed for current 14 Yeti root server. There 
        is a little survey regarding the machine, operation system and DNS software:</t>
    <t> <list style="symbols">
        <t>Machine: 11 out of 14 root server operator using VPS to provide service; </t>
        <t>OS: 6 operators use Linux including ubuntu, Debian, centos. 5 operators use Freebsd 
          and 1 netbsd; and other 2 servers are unknown</t>
        <t>DNS software: 8 our of 14 root server use BIND(vary from 9.9.7 to 9.10.3). 4 of them 
          use NSD （4.10 and 4.15）the other 2 server use knot (2.0.1 and 2.1.0)</t>
      </list></t>
		</section>
    	
    	<section title="Yeti Resolvers and experimental traffic" >

      <t>In client side of Yeti project, we expect participants and volunteers from individual 
        researchers, labs of universities, companies and institutes, and vendors, for example, the DNS 
        software implementers, Developers of CPE devices &amp; IoT devices, middle box developers 
        who can test their product and connect their own testbed into Yeti testbed. Resolver donated 
        by Yeti volunteers are required to be configured with Yeti hint file and Yeti DNSSEC KSK. It is 
        required that Yeti resolver can speak both IPv4 and IPv6, given that not all the stub resolver and 
        authoritative servers are IPv6 capable. </t>

      <t>At the time of writing several university and labs are joined us and contribute certain 
        amount of traffic to Yeti testbed. But it is far from the desired volume of experiment 
        traffic. So there are two alternative way to increase the experimental traffic in Yeti 
        testbed to check the functionality of Yeti root system</t>

      <t>One approach is to mirror the real traffic by off-path method and reply it into Yeti 
        testbed which is already implemented in one of Yeti root server operator. Another approach 
        is to using some traffic generating tool such as Atlas probs to generate specific queries 
        against Yeti server</t>
    	</section>
    </section>

    <section title="Experiments in Yeti testbedd">
    	
    	<t>The main goal of Yeti DNS Project is to act as an experimental network. 
    	We will conduct experiments using this network. In order to make the findings 
    	that result from these experiments more rigorous, we use an experiment 
    	protocol.</t>

    	<t>A Yeti experiment goes through four phases:</t>
        <t> <list style="symbols">
    	<t>Proposal. The first step is to make a proposal. It is discussed and if accepted 
        by the Yeti participants then it can proceed to the next phase.</t>

	     <t>Lab Test. The next phase is to run a version of the experiment in a controlled 
        environment. The goal is to check for problems such as software crashes or protocol 
        errors that may cause failures on the Yeti network, before putting onto the 
        experimental network. </t>

	     <t>Yeti Test. The next phase actually running the experiment on the Yeti network. 
        Details of this will depend on the experiment. It must be coordinated with the Yeti 
        participants.</t>

	     <t>Report of Findings. When completed, a report of the findings of the experiment 
        should be made. It need not be an extensive document. </t>
      </list></t>
       <t> In this section, we are going to introduce some experiments implemented and 
        planned in Yeti project.</t>

    	<section title="Naming scheme and glue issue">
    
      <t>In root server history, naming scheme for individual root server is not fixed. 
      Current IANA Root server adopt [a-m].root-servers.net to represent 13 servers which 
      are labeled with letter from A to M. For example, L root operated by ICANN uses 
      l.root-servers.net to represent their server as NS. One reason behind this naming scheme 
      is that the common suffix 'root-servers.net' can be compressed in DNS massage to 
      produce smaller DNS response.</t>

      <t>Different from IANA root naming scheme, Yeti root system just using separate and normal 
      domain for root servers which is shown in figure 2. The motivation of this naming scheme 
      in Yeti is that it intentionally produces larger packets for priming response for Yeti 
      test reason. However, the change of name scheme not only affect the size of priming 
      response, but also change the content in additional section of the response</t>

      <t>
      When resolver bootstrap, it sends a 'NS-for-dot' query to root serer which is called 
      priming query[]. It looks like this with the "dig" command:</t>

      <t>$ dig @a.root-servers.net -t ns +norecurse +edns +nodnssec .</t>

      <t>Normally in IANA root system the priming response contains the _names_ of the root 
        servers in the answer section and the _addresses_ of the root servers in the additional 
        section. The additional section data is what the resolvers need to actually perform 
        recursion. Shown as below: </t>
          <figure>
  <artwork> 
    <![CDATA[
In priming response : 

Answer Section: 
---------------
.        518400  IN  NS  a.root-servers.net.
.        518400  IN  NS  b.root-servers.net.
  ...
.        518400  IN  NS  m.root-servers.net.

Addtional section: 
------------------
a.root-servers.net.     3600000  IN  A 198.41.0.4
b.root-servers.net.     3600000  IN  A 192.228.79.201
  ...
m.root-servers.net.     3600000  IN  AAAA  2001:dc3::35
          ]]>
  </artwork>
</figure>
      <t>In IANA root system, all the root server returns the "a.root-servers.net" addresses 
        in the additional section, becuase root server not only answer for root zone , but 
        also answer for "root-servers.net" zone. Note that BIND will not behave like this if 
        it is not configured for the "root-servers.net" zone. NSD happily returns such "glue" in 
        the additional section, whether configured for the "root-servers.net" zone or not.</t>
    
      <t>Yeti root naming scheme uses seperate and independent domain for individual root 
        server. It such case, the priming response from Yeti root server will only contain 
        the A&amp;AAAA record of that responding server in BIND9. However we want the Yeti root 
        servers to respond to priming queries with the addresses of all Yeti root servers 
        in the additional section. This will make them operate as similar to the IANA root 
        servers as possible.</t>

      <t>In Yeti root system, there are two approaches adopted in different root servers. 
        One is to patch BIND 9 so that it includes the glue addresses in the additional 
        section. The other one is to add a zone file for each root server and answer for 
        all of them at each Yeti server. That means Each Yeti root server would have a 
        small zone file for "bii.dns-lab.net", "yeti-ns.wide.ad.jp", "yeti-ns.tisf.net", 
        and so on.</t>

    	</section>
    	<section title="Multiple DM">
    		<t>Multiple-Sychn / Multi-ZSK</t>

      <t>According to the Problem statment of Yeti DNS project, more independent participants 
        and operators of root system is desirable. As the name implies, Multi-ZSK mode will 
        introduce different ZSKs sharing a single unique KSK, other than the IANA root system 
        who use single ZSK to sign the root zone. On the condition of good availability 
        and consistency on root system, Multi-ZSK proposal is designed to give each DM operator 
        enough room to manage their own ZSK, by choosing different ZSK, length, duration and 
        even the encryption algorithm etc. </t>

      <t>Accoring to the Yeti experiment protocol, a lab test was done to verify the concept 
        and validity of Multi-ZSK. The purpose of the test are twofolds: 1) To test whether this 
        proposal can be implemented by current DNS protocol &amp;software (to see if there 
        should be some extra modification to protocol or software) 2) To demonstrate the impact 
        of Multi-ZSK proposal to the current root system</t>

       <t>Build a test topology like figure 1, with 2 Root servers and a resolver(BIND) and the 
        hint file of this test only contains the two DM servers. In the first time slot, Root A is 
        up and Root B is turned off. Let resolver bootstrap from Root A and query a certain signed 
        TLD(or junk query). For the second time slot, turn off Root A and turn on Root B. Let resolver shift to Root B to look up another TLD(or junk query). We compare the different time slot to 
        see whether the resolver can validate the DNSSEC signiture</t>
  
            <figure>
  <artwork> 
    <![CDATA[
+---------------+          +---------------+  
|    Root A     |          |     Root B    |  
|   (ZSK A)     |          |     (ZSK B)   |  
+-------+-------+          +--------+------+  
        |                           |        
--------+------------+--------------+--------                       
                     |                        
           +---------+----------+             
           |                    |                      
           |     Resolver       |                        
           +--------------------+  
          ]]>
  </artwork>
</figure>
<t>Figure 3. Multi-ZSK lab test topology</t>

      <t>There are two comparing cases in this test:  </t>
<t> <list style="symbols">
      <t>Case 1: Assign one ZSK to the smart sign process on each DM, which means the root zone 
only contain one single ZSK.</t>

      <t>Case 2: Assign both ZSK A and ZSK B to the smart sign process on each DM, which means the 
root zone contain two ZSK. In this case, it is required that Root A and Root B to share 
their public ZSK to each other before root zone signed.</t>
</list></t>

    <t>In case 1 SERVFAIL is received during switching because the resolve can not validate the 
      signature signed by Root B after switching. In case 2 NOERROR is recieved. It is the actual 
      demo how Multi-ZSK works by adding multiple ZSK to the root zone. As a result, the resolve 
      will cache the key sets instead of single ZSK to validate the data no matter it is sign by 
      Root A or Root B. As follow-up test, unbound also passed the test with morn than 10 DMs and 
      10 ZSKs </t>
    
      <t> Although we can add more DM and ZSK into the test, but adding more ZSKs to root zone 
        will enlarge the DNS response size for DNSKEY queries which may be a concern given the 
        limitation of DNS packet size. Current IANA root server operators are inclined to keep 
        the packets size as small as possible. So the number of DM and ZSK will be parameter which 
        is decided based on operation experience. In Current Yeti root testbed, the parameter is 
        three</t>
    	</section>
    	<section title="Root Renumbering issue and hint file update">
    <t> With the nearest renumbering plan of H root Server's IP address, there is a discussion 
      of ways that resolvers can update their hint file. Traditional ways includes using ftp protocol 
      by doing a wget and dig servers address manually. Each way would depend on operators 
      manual operation. As a result, there is many old machines could not update its hint-file 
      rightly. As a proof, after done renumbering for thirteen years, there is an observation 
      that the "Old J-Root" can still receives DNS query traffic[OARC slides].</t>

    <t>This experiment proposal aims to find a automatic way for hint-file updating. The 
      already-done works is a shell script tool which provides the function that update a hint-file 
      in file system automatically with DNSSEC and trust anchor validation</t>


    <t>The methodology is straightforward. The tool first queries the NS list for "." domain 
      and queries A and AAAA record for every domain on the NS list. It requires DNSSEC validation 
      for both signiture and trust anchor for all the answers. After getting all the answers, 
      the tool compares the new hint file to the old one. If there is a difference, it renames 
      the old one with a timestamp and replace the old one with the new one. Otherwise the tool 
      deletes the new hintfile and nothing will be changed.</t>

    <t>Note that in current IANA root system the root NS record is not signed due to lack of 
      incentive. So the tool can not fully work in the production network. In Yeti root 
      system it is required that NS record should be signed which provides perfet test 
      envieroment for such proposal.</t>

    	</section>
    	<section title="DNS fragements">

      <t>In considertion for new DNS protocal and operation, there is always a hard limit 
        on the DNS packts size. Take Yeti for example: adding more root severs, Yeti naming 
        Scheme, rolling the KSK and Multi-ZSK increase the packets size with no doubt. The 
        fear of large DNS packets mainly stem from tow aspects: one is IP-fragements[] and 
        the other is frequenetly falling back to TCP.</t>

      <t>In Yeti testbed we implement a mechnism which supports larger DNS packet working 
        around the IP-layer fragment caused by middle box misbehavior(in IPv4) and IPv6 MTU 
        limitation by splitting a single DNS message across multiple UDP datagrams. This DNS 
        fragements mechanism is documented in [draft-muks-dns-message-fragments] as an 
        experimental IETF draft.</t>

    	</section>
    </section>

    <section title="Other Technical findings and bugs">
    	<t>IPv6 fragments</t> 

    	<t>KSK rollover</t>

   
    	<t>	Root compression</t>

    </section>

    <section title="Open issues">
    	<t>Other Naming experiment  etc</t>
    
    </section>
  
  </middle>

  <back>

    <references title="References">
      &RFC1035;&RFC1123; &RFC5681; &RFC6891;&RFC3542;
      &I-D.dnsop-cookies;&I-D.dnsop-respsize;

    <reference anchor="Fragment-Poisonous">
	            <front>
	                <title>Fragmentation Considered Poisonous</title>
			<author fullname="Herzberg, A."  initials="A." surname="Herzberg"></author>
			<author fullname=" H. Shulman"  initials="H." surname="Shulman"></author>
			<date year="2012" />            
	            </front>
	   </reference>
	   <reference anchor="SAC016">
            <front>
                <title>Testing Firewalls for IPv6 and EDNS0 Support</title>
                <author>
                    <organization>ICANN Security and Stability Advisory Committee</organization>
                </author>
                <date year="2007" />
            </front>
     </reference>
     
     <reference anchor="T-DNS" target="http://www.isi.edu/~johnh/PAPERS/Zhu14b.pdf">
	        <front>
	                <title>T-DNS: Connection-Oriented DNS to Improve Privacy and Security (extended)</title>
			<author fullname="Liang Zhu"  initials="L" surname="Zhu"></author>
			<author fullname="Zi Hu"  initials="Z" surname="Hu"></author>
			<author fullname="J. Heidemann"  initials="J." surname="Heidemann"></author>
			<date year="2007" />            
		</front>
	</reference>
		   <reference anchor="SAC035">
            <front>
                <title>DNSSEC Impact on Broadband Routers and Firewalls </title>
                <author>
                    <organization>ICANN Security and Stability Advisory Committee</organization>
                </author>
                <date year="2008" />
            </front>
     </reference>
     </references>

    <section title="Change History (to be removed before publication)">
      <t>
        <list style="symbols">

          <t>
	  draft-dnsop-dns-message-fragments-00
          <vspace/>
          Initial draft.
          </t>

        </list>
      </t>
    </section>

  </back>
</rfc>

